LIBRARIES

https://github.com/JuliaText/Word2Vec.jl
https://github.com/JuliaText/Embeddings.jl

BIBLIO

https://nlp.stanford.edu/projects/glove/
Manning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S., and Mc- Closky, D. (2014). The Stanford CoreNLP natural language processing toolkit. ACL


CATEGORÍAS
- Por perfil de usuario: medio, pro, ..

CLASIFICACIÓN

Clasificación de intento (queries de cliente): clase, frase del cliente, ¿respuesta obtenida en el live chat?
Clasificación de respuestas: clase, respuesta.
Relación entre clasificación de respuestas y clasificación de preguntas: mismatch.

Identificación de pares question/answer, proposal/acceptance. Estos pares los podemos asociar a intents.
Podríamos hacer un diccionario de intents y n-grams o frases asociados.
Identificación de preguntas aclaratorias: por ejemplo, aquellas preguntas seguidas de otra pregunta donde se repite parte de la anterior.

Clasificación de conversaciones.
Varios niveles de clasificación: conversación, frase/intent, par de frases consecutivas, …

Para las tareas de clasificación: reg. expresions que contienes palabras-clave.

ANÁLISIS DISTRIBUTIVO

Conversation
--  Turns: number, length (average, median,..)

PREDICCIÓN ÉXITO ARGUMENTARIO 

Podríamos aplicar un random forest a una conversación, donde ésta se define por un conjunto de variables:
Presencia / tipo de URLs.
Tipo de frases intents
Longitud
Frase inmediatamente anterior del operador a la sugerencia de URL
Frase inmediatamente anterior del cliente a la sugerencia de URL
….
La variable a predecir sería ‘éxito’ en el intento o tipo de ticket de la conversación.

SEMANTIC SIMILARITY

We represent a document by taking the (tf-idf) vectors of all the words in the document, and computing the centroid of all those vectors.
The centroid is the multidimensional version of the mean; the centroid of a set of vectors is a single vector that has the minimum sum of squared distances to each of the vectors in the set. 

PREGUNTA - RESPUESTA

Tomemos el punto de partida de un argumentario, como un conjunto de preguntas. Para cada pregunta, encontremos las posibles respuestas.
Las mejores respuestas son aquellas que tienen un coseno más alto con el vector tf-idf de la pregunta.
Hay que computar una matriz de términos - respuestas, con los tf-idf de cada término en cada respuesta,
y otra o la misma con los tf-idf de cada término en cada pregunta. Una pregunta se define como los tf-idf de los términos ‘en la dimensión-columna’ de esa pregunta-documento.

Otro punto para evaluar el argumentario: distancia (coseno) entre las frases - términos de las preguntas en el argumentario y las preguntas reales de los clientes.
Cuanto menos la distancia, menor la necesidad de improvisar del operador. —-> CALIDAD DE LOS INTENTS DEL ARGUMENTARIO

La similitud entre las respuestas a un mismo intent y la respuesta propuesta en el argumentario, da una idea de la calidad de las respuestas reales.
La similitud entre las respuestas reales da una medida de la variabilidad de las respuestas a un mismo intent.

Otra opción para no utilizar tf-idf es clasificar manualmente distintas preguntas reales como alguno de los intents del argumentario.
Podríamos entonces ver qué intents (preguntas reales clasificadas) tienen mayor uniformidad de respuestas haciendo un análisis con tf-idf.

Con los tf-idf podríamos hacer clusters de preguntas y respuestas y ver cómo se ajustan a lo previsto en el argumentario.
El coseno medio (medido como la media del coseno de cada respuesta con la pregunta) podría dar una idea de uniformidad interna del cluster, además de los centroides.

Un intent con varios wordings podría utilizarse como varios documentos: las respuestas tenderían a tener similares tf-idf con las diferentes versiones del intent.
En una conversación, un alto coseno entre dos preguntas o dos respuestas puede entenderse como que nos mantenemos en el mismo intent.
Podríamos contar el número de intents de una conversación, el número de turnos dedicados a un intent o a una clase de intent, …Tener estructurada una conversación
en una secuencia de intents y sus repeticiones podría servirnos para:
Saber que intents tienen más repeticiones en una misma conversación. Dificultad de entendimiento? Complejidad del intent?

Cómo probar un nuevo cuestionario: calcular el scoring de las nuevas preguntas contra las respuestas de la última campaña: cómo serían respondidas.

Diferenciar entre intents y dialog acts (la clasificación en el otro doc de .pages).

ARGUMENTARIOS DE CONTRASTE

Doble lista: de intents, de actos de habla o diálogo, tanto en la parte de cliente como de operador.
Análisis:
Utilizamos los intents + acto de diálogo como queries contra la base de las conversaciones habidas.
Sacamos ejemplos dentro de un rango de similaridad mínimo: las mejores y las peores respuestas.
Utilizamos los intents + acto de diálogo para clasificar los tipos de intents + actos habidos realmente en la parte de clientes.
Dos opciones de clasificación: por coseno o bayesiana.

Construimos, también, las respuestas óptimas para cada intent-acto y obtenemos medidas de distancia entre las respuestas reales y las propuestas.
(Esta parte serviría para valorar nuestro método: añadimos las respuestas ideales a la base de datos de respuestas reales y vemos 
en qué medida son seleccionadas en nuestra simulación de query-respuesta.

Los análisis han de hacerse en el nivel de desagregación mínimo de intento, o intent-acto.
Podemos hacer los dos tipos de clasificaciones de preguntas y respuestas siempre de dos formas: bayesiana y por coseno.
Estadística de buenas respuestas: alto coseno, en el grupo de buenas respuestas.
Dos pistas para sacar la mejor respuesta: similitud entre pregunta ficticia y real, similitud entre pregunta ficticia y respuesta real.
Podemos crear un índice sintético con ambas. O jugar con diferentes ponderaciones para una y otra: 0.7 * similitud-preguntas + 0.3 similitud-pregunta-respuesta.

Clasificación de estadios finales satisfactorios en cada tipo de intent: manuales para clasificación bayesiana, o automáticos basados en ciertos eventos
identificables en las conversaciones (sugerencia de URL, aceptación resolución incidencia, ….)
Conjunto de respuestas que debería haber respondido el operario: de 1 a 3 respuestas o malas respuestas por pregunta.
Tipo de acción: datos (información sobre el cliente u operador, quizás), suggestion, instructions, options, pregunta de aclaración?,.… 
Operador: sugerencia o recomendación, instrucción, opciones,  recabar información. 
Cliente: exposición o suministro de información, aclaración (con pregunta), resolución,, .. 
Comunes: apertura o cierre diálogo, denegación, confirmación, petición, preguntas de aclaración …

QUÉ INCLUIR sobre dialog policy: al menos que habría que decir algo al respecto, pero ahora no lo hacemos.
La mayor o menor presencia de ciertos actos de habla puede ser indicativa de deficiencias en el argumentario.

INBENTA

Inbenta can map all these negative signals and discover where customers have had similar inquiries but did not receive a satisfactory response.
These negative responses alert the company to crucial new material that needs to be created in order to answer their customers’ questions. 

“What time should I arrive?”
“When do gates close?”
“Boarding time”
These questions or statements all use different words or phrases but mean the same thing. Inbenta’s Semantic Clustering is able 
to group all of these responses together into a cluster with the same meaning – essentially when the passenger should arrive at the airport for their flight.
